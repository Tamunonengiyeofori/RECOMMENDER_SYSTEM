{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT ALL THE REQUIRED LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse\n",
    "import nltk\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import joblib\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING\n",
    "READ THE DATASET AND NAME THE SELECTED COLUMNS IN CORRECT FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATE PREPROCESSING\n",
    "# Accessing the dataset and picking \n",
    "elect_data = pd.read_csv('electronics_dataset.csv')\n",
    "\n",
    "\n",
    "# Exploratory Data Analysis to show some information about the overall dataset\n",
    "\n",
    "# view the first ten columns from the dataset\n",
    "# elect_data.head(10)\n",
    "\n",
    "# view the number of rows and columns in the dataset\n",
    "# elect_data.shape\n",
    "\n",
    "# view the datatypes of the data columns present in the dataset\n",
    "# elect_data.dtypes()\n",
    "\n",
    "# view the other information about the dataset\n",
    "# elect_data.describe()\n",
    "# elect_data.info()\n",
    "\n",
    "# elect_data.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new datset with only the columns that are highly correlated and relevant to the analysis\n",
    "new_df = elect_data[['id','brand','categories',\"reviews.rating\",\"reviews.text\",'reviews.username']]\n",
    "print(type(new_df))\n",
    "\n",
    "# Exploratory Data Analysis to show some information about the new dataset\n",
    "\n",
    "# view the first ten columns from the dataset\n",
    "new_df.head(10)\n",
    "\n",
    "# view the number of rows and columns in the dataset\n",
    "# new_df.shape\n",
    "\n",
    "# view the datatypes of the data columns present in the dataset\n",
    "# new_df.dtypes\n",
    "\n",
    "# view the other information about the dataset\n",
    "# THE distribution of the ratings which are numerical varaibles\n",
    "# new_df.describe()\n",
    "# new_df.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE DATASET HAS SOME NULL VALUES THAT ARE NOT IMPORTANT FOR OUR ANALYSIS. SO WE ANALYZE THE DISTRIBUTION OF THE NULL VALUES ACROSS THE COLUMNS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values in the dataset and store in a variable \n",
    "null_colls = new_df.isnull()\n",
    "null_colls.sum()\n",
    "\n",
    "# visializing the null values in the dataset with a heatmap\n",
    "sns.heatmap(null_colls,yticklabels=False,cbar=False,cmap='viridis')\n",
    "\n",
    "\n",
    "\n",
    "# Replacing the null values with the mean of the column\n",
    "# for col in num_col:\n",
    "#     elect_data[col] = pd.to_numeric(elect_data[col])\n",
    "#     elect_data[col] = elect_data[col].fillna(elect_data[col].mean(), inplace=True)\n",
    "\n",
    "# elect_data.head()\n",
    "\n",
    "# Replace the null values with the 'NaN' string\n",
    "# for col in null_col:\n",
    "#     elect_data[col] = elect_data[col].fillna('NaN', inplace=True)\n",
    "\n",
    "# print(elect_data.head(10))\n",
    "\n",
    "\n",
    "# # Drop the columns with the highest number of null values as they are irrelevant to the analysis\n",
    "# elect_data.drop(null_col, axis=1, inplace=True)\n",
    "\n",
    "# print(elect_data.head(5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# new_df.to_csv('new_electronics_dataset.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIEW THE DISTRIBUTION OF THE RATINGS ACROSS THE ELECTRONIC PRODUCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "fig = sns.catplot(data=new_df, x=\"reviews.rating\",aspect=2.0, kind='count')\n",
    "fig.set_ylabels(\"Ratings across products\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "\n",
    "# # create a correlation matrix of the dataset\n",
    "# data_corr = df.corr()\n",
    "# sns.heatmap(data_corr, annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE CAN SEE THAT THE DISTRIBUTION OF THE RATINGS IS NOT EVEN. SOME PRODUCTS HAVE MORE RATINGS THAN OTHERS. THIS IS BECAUSE SOME USERS HAVE RATED MULTIPLE PRODUCTS AND VICE-VERSA SO WE NEED TO CHECK THE NUMBER OF UNIQUE USERS WHO HAVE RATED THE PRODUCTS AND ALSO THE NUMBER OF UNIQUE PRODUCTS THAT HAVE BEEN RATED BY THE USERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of actual users in the dataset is:  6493\n",
      "The number of unique products in the dataset is:  50\n"
     ]
    }
   ],
   "source": [
    "# find the unique user reviews\n",
    "unique_user_reviews = np.unique(new_df['reviews.username'])\n",
    "\n",
    "# find the length of the unique user reviews\n",
    "len_unique_user_reviews = len(unique_user_reviews)\n",
    "\n",
    "print(\"The number of actual users in the dataset is: \", len_unique_user_reviews)\n",
    "\n",
    "# find the unique products\n",
    "unique_products = np.unique(new_df['id'])\n",
    "\n",
    "# find the number of unique products\n",
    "no_unique_products = len(unique_products)\n",
    "\n",
    "print(\"The number of unique products in the dataset is: \", no_unique_products)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO TRAIN THE MODEL, THE REVIIEWS TEXT NEEDS TO BE PREPROCESSED AND CONVERTED INTO A SUITABLE FORMAT FOR THE COMPUTER TO USE IT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the users' reviews in a variable\n",
    "user_reviews = elect_data['reviews.text']\n",
    "\n",
    "reviews_text = elect_data.iloc[: , [21]]\n",
    "\n",
    "# The reviews text are contained in the 21st column of the dataset so all the other columns will be dropped\n",
    "print(reviews_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two files to store the input dataset and resulting output dataset\n",
    "data_input = \"electronics_dataset.csv\"\n",
    "data_output = \"output_data.csv\"\n",
    "\n",
    "# create a new dataframe with only the reviews text column and store it in a csv file\n",
    "df = elect_data.loc[: , [\"reviews.text\"]]\n",
    "df.to_csv('output_data.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cols_to_remove = [0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,22,23,24,25,26]\n",
    "\n",
    "# cols_to_remove = sorted(cols_to_remove, reverse=True) # Reverse so we remove from the end first\n",
    "# row_count = 0 # Current amount of rows processed\n",
    "\n",
    "# with open(data_input, \"r\") as source:\n",
    "#     reader = csv.reader(source)\n",
    "#     with open(data_output, \"w\", newline='') as result:\n",
    "#         writer = csv.writer(result)\n",
    "#         for row in reader:\n",
    "#             row_count += 1\n",
    "#            # print('\\r{0}'.format(row_count), end='') # Print rows processed\n",
    "#             for col_index in cols_to_remove:\n",
    "#                 del row[col_index]\n",
    "#             writer.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT THE REVIEWS CSV FILE INTO A TEXT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"reviews.txt\", \"w\", encoding=\"utf-8\") as new_file:\n",
    "    writer = csv.writer(new_file)\n",
    "    # for val in df[\"reviews.text\"]:\n",
    "    #     writer.writerow([val])\n",
    "    \n",
    "    with open(\"output_data.csv\", encoding=\"utf-8\") as old_file:\n",
    "        for row in csv.reader(old_file):\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    # with open(\"output_data.csv\") as old_file:\n",
    "    #     [new_file.write(\" \".join(row)+'\\n') for row in csv.reader(old_file)]\n",
    "\n",
    "    # new_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO MAKE THE REVIEWS TEXT MORE MEANINGFUL TO A MACHINE, THE STOPWORDS ARE REMOVED USING THE NLTK LIBRARY AS THEY ADD LITTLE MEANING TO THE OVERALL TEXT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "with open(\"output_data.csv\", \"r\", encoding=\"utf-8\") as csv_file, open(\"stpw_deleted.csv\", \"w\", newline='', encoding=\"utf-8\") as stop_word_file:\n",
    "    word_writer = csv.writer(stop_word_file)\n",
    "    word_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "    stpw_counter = 0\n",
    "    for lines in word_reader:\n",
    "        for txt_line in lines:\n",
    "            txt_words = txt_line.split()\n",
    "            filtered_words = [w for w in txt_words if w.lower() not in stop_words]\n",
    "            stpw_counter += len(txt_words) - len(filtered_words)\n",
    "            word_writer.writerow(filtered_words)\n",
    "       \n",
    "print(\"The number of stopwords in the dataset is: \", stpw_counter)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE NEXT STEP IS TO SHARE THE SENTENCES OF THE REVIEWS INTO DIFFERENT PARTS OF SPEECH USING NLTK'S PARTS OF SPEECH TAGGER(POS TAGGING). THIS WILL HELP THE MACHINE TO UNDERSTAND THE MEANING OF THE SENTENCES BETTER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_data.csv', \"r\", encoding=\"utf-8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for tokens in csv_reader: \n",
    "        print(\"POS::\")\n",
    "        tagged_tokens = nltk.pos_tag(tokens)\n",
    "        print(tagged_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO MAKE THE WORDS IN THE REVIEWS TEXT EVEN MORE MEANINGFUL TO THE MACHINE, WE USE STEMMING TO REDUCE THE WORDS TO THEIR ROOT FORMS. STEMMING IS A PROCESS OF REDUCING THE WORDS TO THEIR ROOT FORMS BY REMOVING THE ENDINGS OF THE WORDS. FOR EXAMPLE, THE WORDS 'RUNNING', 'RUNS', 'RUNNER' ARE ALL REDUCED TO THE ROOT FORM 'RUN'. THIS WILL HELP THE MACHINE TO UNDERSTAND THE MEANING OF THE WORDS BETTER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Porter Stemmer object\n",
    "# porter = PorterStemmer()\n",
    "\n",
    "# with open('electronics_dataset.csv', \"r\", encoding=\"utf-8\") as csv_file:\n",
    "#     csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "#     print(\"Porter Stemmer::\")\n",
    "#     for lines in csv_reader: \n",
    "#         for word in lines:\n",
    "#             print(word, porter.stem(word))\n",
    "d=0\n",
    "porter = PorterStemmer()\n",
    "with open('electronics_dataset.csv', \"r\", encoding=\"utf-8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for lines in csv_reader: \n",
    "        txt_line = csv_file.read()\n",
    "        txt_words = txt_line.split()\n",
    "print(\"Porter Stemmer::\")\n",
    "for check_word in txt_words:\n",
    "    print(check_word,porter.stem(check_word))\n",
    "\n",
    "# porter = PorterStemmer()\n",
    "\n",
    "# with open('electronics_dataset.csv', \"r\", encoding=\"utf-8\") as csv_file:\n",
    "#     csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "#     for lines in csv_reader: \n",
    "#         word_tokens = word_tokenize(lines[0])\n",
    "#         for word in word_tokens:\n",
    "#             print(word, porter.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the pool of english stopwords in a variable\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # read the reviews text csv file \n",
    "# with open (\"output_data.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
    "#     word_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "#     for lines in word_reader:\n",
    "#         txt_line = csv_file.read()\n",
    "#         txt_words = txt_line.split()\n",
    "        \n",
    "# stpw_counter = 0\n",
    "        \n",
    "# # add all words that are not stopwords to a list      \n",
    "# for w in txt_words:\n",
    "#     if w.lower() not in stop_words:\n",
    "#         stop_word_file = open(\"stpw_deleted.csv\", \"a\")\n",
    "#         stop_word_file.write(\" \" + w)\n",
    "#         stop_word_file.close()\n",
    "#     else:\n",
    "#         stpw_counter += 1\n",
    "       \n",
    "# print(\"The number of stopwords in the dataset is: \", stpw_counter)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE NEXT STEP IS TO DROP THE REVIEWS COLUMN FROM THE DATSET AS IT IS NOT REQUIRED FOR BUILDING THE RECOMMENDER SYSTEM BASED ON COLLABORATIVE FILTERING FROM SILMILAR USER REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elect_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
